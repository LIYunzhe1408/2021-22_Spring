<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
	<title>无标题文档</title>
	<link href="img/style.css" rel="stylesheet" type="text/css" media="all" />
</head>

<body>
	<div class="mainbody quote-report">
        <strong class="title">引用片段详情</strong>
        <table class="custom" width="100%">
        	<thead>
        		<tr><td width="10%">序号</td><td>引用片段</td></tr>
        	</thead>
        	<tbody>
        			        			<tr>
	        				<td align="center" valign="middle">1</td>
	        				<td valign="top">本文灵感来源于：近期对NLP和CV领域创新项目的学习中阅读到的一篇论文 Learning Transferable Visual Models From Natural Language Supervision[1]</td>
	        			</tr>
	        			<tr>
	        				<td align="center" valign="middle">2</td>
	        				<td valign="top">直接从原始的文本中预训练模型已经在过去几年的NLP领域中，取得了革命性的成果，比如：BERT[2]</td>
	        			</tr>
	        			<tr>
	        				<td align="center" valign="middle">3</td>
	        				<td valign="top">比如OpenAI自己的GPT[3]</td>
	        			</tr>
	        			<tr>
	        				<td align="center" valign="middle">4</td>
	        				<td valign="top">但在视觉领域，一般的做法还是在ImageNet[4]</td>
	        			</tr>
	        			<tr>
	        				<td align="center" valign="middle">5</td>
	        				<td valign="top">过去20年中，相关工作有大跨度的进展，论文作者团队开发的CLIP（Contrastive Language-Image Pre-Training）模型的工作和Li et al(2017)[5]</td>
	        			</tr>
	        			<tr>
	        				<td align="center" valign="middle">6</td>
	        				<td valign="top">而VirTex[6]</td>
	        			</tr>
	        			<tr>
	        				<td align="center" valign="middle">7</td>
	        				<td valign="top">例如Zhang et al(2020)[7]</td>
	        			</tr>
	        			<tr>
	        				<td align="center" valign="middle">8</td>
	        				<td valign="top">, Gomez et al(2017)[8]</td>
	        			</tr>
	        			<tr>
	        				<td align="center" valign="middle">9</td>
	        				<td valign="top">以及Joulin et al(2016)[9]</td>
	        			</tr>
	        			<tr>
	        				<td align="center" valign="middle">10</td>
	        				<td valign="top">模型训练方面，作者在视觉模型上训练了8个模型，每个模型基于Adam优化器（Kingma & Ba, 2014)[10]</td>
	        			</tr>
	        			<tr>
	        				<td align="center" valign="middle">11</td>
	        				<td valign="top">, 2017)[11]</td>
	        			</tr>
	        			<tr>
	        				<td align="center" valign="middle">12</td>
	        				<td valign="top">, 2016)[12]</td>
	        			</tr>
	        			<tr>
	        				<td align="center" valign="middle">13</td>
	        				<td valign="top">、半精度Adam统计(Dhariwal et al,2020)[13]</td>
	        			</tr>
	        			<tr>
	        				<td align="center" valign="middle">14</td>
	        				<td valign="top">但相较于最新最大的Vision Transformer[14]</td>
	        			</tr>
	        			<tr>
	        				<td align="center" valign="middle">15</td>
	        				<td valign="top">和Noisy Student[15]</td>
	        			</tr>
        	</tbody>
        </table>
    </div>
</body>
</html>
