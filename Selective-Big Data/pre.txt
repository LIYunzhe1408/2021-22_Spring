基于自然语言有监督的信号训练视觉模型，实现图像分类的效果。
研究论文是Learning Transferable Visual Models From Natural Language Supervision，作者是OpenAI团队
过去几年中，NLP领域的发展很显著，使用了将预训练和下游任务分离的模式，
使得模型只需要极少量的下游数据、或不需要下游数据就可以处理任务。我们称为........这也是衡量模型好坏的一大指标。也是泛化性的体现。

自然的想法是说，能不能把NLP的这种方法用在视觉领域，过去20年对于这种方法的迁移一直有所研究，
在几篇研究中，N-Grams和本篇论文采用类似的方法：
这种方法简单来说，就是首先输入你想识别到的类的文本信息，例如：馒头、馄饨、小笼、上海、面粉；
再输入需要识别的图像信息：【图片】，
将二者分别提取特征进行相似度计算。
计算所得最大值的文本，就是这张图片的解释。
，但由于没有Transformer和大数据集，在ImageNet上的准确度很低。

因此，OpenAI团队在本篇研究中就以上两点问题进行改进，他们采用的方法可以由这张图简单概括：

首先预训练一个对比学习的模型，对比文本和图像的特征，因此该模型命名为CLIP（contrastive language-image pre-training)
考虑过完形填空的方式预测文本信息，但一幅图的解释可能有很多种，比如一个人在打字，。。。训练会非常耗时，因此采用对比学习的方式，匹配到，就成功。

文本方面采用Text Transformer
视觉方面训练了8个模型包括了ResNet和Vitsion Transformer，差异在规模，精度也和规模是呈现正相关的。
团队创建了一个有四个亿图片-文本对样本的数据集用来预训练，其中有很多优化的点，才使得其能正常训练，这里不过多介绍细节。


预训练后就可以投入使用了，
投入使用的方法同样如图，先输入5个句子或单词，会被PE为5句话，再编码后提取出5个文本特征，输入图片，提取特征，进行特征计算。
得出概率分布。即：。。。
投入使用时，将匹配的文本通过prompt engineering变为一句话，这种方法起到了提示的作用，也是当下很火热的研究方向之一。
作用是去除文本中可能产生二义的可能，比如识别一个遥控器，仅一个单词可能被识别为遥远的，就会使下游任务中识别出错；


作者做了特别多的实验，分别测试了27个数据集的各种shot下的性能，
前文提到zero-shot，不用一张测试数据的图片，就可以达到其他模型用1.8万张测试图片训练的效果，甚至高于one-shot 和 two-shoot。

这篇研究的价值在于打通了NLP和CV的方法壁垒，用出奇简单的方法得到了出奇出色的结果。
此篇文章一出，各类由CLIP引发的应用接踵而至。
可以从以下三个角度评价这篇论文：
1. 新颖性
2. xxx
3. 问题规模

在验证时也发现了一些局限性。。。。。。
给后续的研究留下了提升的空间。
希望大家



